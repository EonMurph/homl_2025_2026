{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec02c415-becb-4625-bd39-7eea8a052bb2",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac60507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7469813b-e0c5-45b1-8f34-c4b60d89fcbe",
   "metadata": {},
   "source": [
    "## Our running example - a corpus of three documents (tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139d0d44-21c7-48c6-af92-748565c04458",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\n",
    "    \"No one is born hating another person because of the color of his skin or his background or his religion.\",\n",
    "    \"People must learn to hate, and if they can learn to hate, they can be taught to love.\",\n",
    "    \"For love comes more naturally to the human heart than its opposite.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09443715-1662-49cf-bed0-ce400db8887d",
   "metadata": {},
   "source": [
    "## Tokenizing, discarding stop words, count vectorization - one class does it all!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8777c27c-156e-4f6d-848a-c89fb4ec6d49",
   "metadata": {},
   "source": [
    "In the next lecture, we'll put this into a pipeline with a classifier.\n",
    "\n",
    "By default, it converts to lowercase, it treats punctuation as spaces, and it treats two or more consecutive characters as a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19252902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vectorizer\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Run the vectorizer\n",
    "vectorizer.fit(tweets)\n",
    "X = vectorizer.transform(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2716a77-ef4a-4b1b-a44a-c77ebae2e595",
   "metadata": {},
   "source": [
    "FYI here's the (somewhat strange) list of stop-words that scikit-learn uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ef0291-99a5-48c0-a41d-8e25e64ad674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "print(ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa9bac5-fc74-40d3-a0c2-8ae015f89023",
   "metadata": {},
   "source": [
    "But in the CountVectorizer we can say stop_words = None or we can supply our own list of stop-words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb59e625-c614-4f8a-a397-84144a4b91db",
   "metadata": {},
   "source": [
    "FYI, let's see the tokens that it ends up with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6a0d58-d0f8-4756-80c3-963333277196",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf282a7-a390-495e-a50d-b9f94cdeda33",
   "metadata": {},
   "source": [
    "Suppose we wanted to do stemming. There is no stemmer in scikit-learn. But there are stemmers in `nltk`, e.g. `nltk.stem.snowball.EnglishStemmer`. It's a little tricky because we have to make sure that we remove stop-words before we stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd124af0-394a-44de-9440-ba9d59c20205",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "        \n",
    "    def build_tokenizer(self):\n",
    "        tokenizer = super().build_tokenizer()\n",
    "        stemmer = EnglishStemmer()\n",
    "        if self.get_stop_words():\n",
    "            return lambda doc: (stemmer.stem(t) for t in tokenizer(doc) if t not in self.get_stop_words())\n",
    "        return lambda doc: (stemmer.stem(t) for t in tokenizer(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff99108-181e-4b9a-8450-78a6bc3b0763",
   "metadata": {},
   "source": [
    "Now we can use StemmedCountVectorizer instead of CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9be0a76-0055-4394-b878-fcb7167dca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vectorizer\n",
    "vectorizer = StemmedCountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Run the vectorizer\n",
    "vectorizer.fit(tweets)\n",
    "X = vectorizer.transform(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea48fd03-5444-44af-8986-402b4a23af02",
   "metadata": {},
   "source": [
    "We can see that the tokens are now different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f89109f-0052-4e33-9f11-cad1d7e2ff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4ff510-1e4c-45b0-b892-0f81d21b8368",
   "metadata": {},
   "source": [
    "We can look at the sparse array. The first number identifies the tweet (0, 1 or 2), the second is which token, and the last is the frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5329d5aa-c9d7-4291-8629-99b686a8450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d65e357-3d2f-4f88-9762-b4848e182fbe",
   "metadata": {},
   "source": [
    "Let's vectorize a new document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6231cb50-66b0-4d0d-ad71-67592f66b0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_document = \"Unsurprisingly, people hate to learn that their religion loves to hate.\"\n",
    "\n",
    "new_document_as_vector = vectorizer.transform([new_document])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c7878d-165c-497b-a9db-71f608513ee5",
   "metadata": {},
   "source": [
    "Notice how it ignores words that weren't in the original tweets, such as \"unsurprisingly\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92a7c82-aa17-4e9f-aacd-863290a6b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_document_as_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df17b621-fcb4-4f28-9e30-c289ec97864a",
   "metadata": {},
   "source": [
    "## Tokenizing, discarding stop-words, TF-IDF vectorization - there's a class that does all this instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332e2fc5-c6ce-4929-9908-8e34295ad1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Run the vectorizer\n",
    "vectorizer.fit(tweets)\n",
    "X = vectorizer.transform(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d663eb-a7ae-461a-a256-f62ffbb77799",
   "metadata": {},
   "source": [
    "We could create a version that does stemming again, if we wanted. But we won't bother here.\n",
    "\n",
    "Here are the tokens. They're different because we didn't stem this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14686dd2-3d47-48c1-85a4-b6472f2df31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d848b9b-8537-4a57-a22f-18ade3c2da47",
   "metadata": {},
   "source": [
    "Here are the tf-idf scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de7259-0390-444f-864d-3eede16109bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eb0df7-f3db-4dc5-8c57-68cb0404d19b",
   "metadata": {},
   "source": [
    "## Unigrams, Bigrams and Both"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f11424-1d00-4ca2-b383-610713230700",
   "metadata": {},
   "source": [
    "Up to now, our tokens are unigrams. \n",
    "\n",
    "This is what we get if we use bigrams instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f5cfbd-b289-4240-94b9-52300eaa5cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "# Run the vectorizer\n",
    "vectorizer.fit(tweets)\n",
    "X = vectorizer.transform(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22955e11-dee3-4d4b-9333-2adfaf717ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ea92de-8a28-46f5-a06e-60ab082dc3f6",
   "metadata": {},
   "source": [
    "Note that we are less likely to discard stop-words or to do stemming in this case.\n",
    "\n",
    "More common if you are using bigrams is to allow unigrams as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba36326-d647-41aa-b599-091a261e6354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "# Run the vectorizer\n",
    "vectorizer.fit(tweets)\n",
    "X = vectorizer.transform(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789d3c0c-a914-4c6f-9e9d-76e83a6fc524",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6055a66-7f95-45f4-a5a3-dd234f1d62ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
