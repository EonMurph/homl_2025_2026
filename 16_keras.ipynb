{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from keras import Model\n",
    "from keras import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Normalization\n",
    "from keras.layers import Rescaling\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running on Google Colab, uncomment the next line before executing this code cell.\n",
    "\n",
    "# ! pip install keras_tuner\n",
    "\n",
    "import keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in datasets and split them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    base_dir = \"./drive/My Drive/Colab Notebooks/\" # You may need to change this, depending on where your notebooks are on Google Drive\n",
    "else:\n",
    "    base_dir = \".\"\n",
    "dataset_dir = os.path.join(base_dir, \"datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing = pd.read_csv(os.path.join(dataset_dir, \"housing.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"BasementArea\", \"GroundFloorArea\", \"Bedrooms\", \"Condition\"]\n",
    "\n",
    "X_housing = df_housing[features].values\n",
    "y_housing = df_housing[\"SalePrice\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_housing, X_test_housing, y_train_housing, y_test_housing = \\\n",
    "    train_test_split(X_housing, y_housing, test_size=0.2, random_state=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cs1109 = pd.read_csv(os.path.join(dataset_dir, \"cs1109.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"lect\", \"lab\"]\n",
    "\n",
    "X_cs1109 = df_cs1109[features]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_cs1109 = label_encoder.fit_transform(df_cs1109[\"outcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cs1109, X_test_cs1109, y_train_cs1109, y_test_cs1109 = \\\n",
    "    train_test_split(X_cs1109, y_cs1109, test_size=0.2, stratify=df_cs1109[\"outcome\"], random_state=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (a dictionary) and get the features DataFrame and target values from the dictionary\n",
    "iris = load_iris(as_frame=True)\n",
    "iris_df = iris.data\n",
    "iris_y = iris.target\n",
    "\n",
    "# Split off the test set: 20% of the dataset.\n",
    "train_iris_df, test_iris_df, train_iris_y, test_iris_y = \\\n",
    "    train_test_split(iris_df, iris_y, train_size=0.8, stratify=iris_y, random_state=rng)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train_iris = train_iris_df.values\n",
    "y_train_iris = train_iris_y.values\n",
    "X_test_iris = test_iris_df.values\n",
    "y_test_iris = test_iris_y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression on Housing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Normalization layer standardizes (scales) the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(4,))\n",
    "x = Normalization()(inputs)\n",
    "x = Dense(units=16, activation=\"relu\")(x)\n",
    "x = Dense(units=8, activation=\"relu\")(x)\n",
    "outputs = Dense(units=1, activation=\"linear\")(x)\n",
    "housing_model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_model.compile(optimizer=RMSprop(learning_rate=0.001), loss=\"mse\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_model.fit(X_train_housing, y_train_housing, epochs=40, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_mae = housing_model.evaluate(X_test_housing, y_test_housing)\n",
    "test_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should edit the code and experiment: e.g. add or remove hidden layers, change the number of neurons in the hidden layers, change ReLU to sigmoid, change from RMSprop to another optimizer, change the learning rate, change the number of epochs, or change the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Binary Classification on Student Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(2,))\n",
    "x = Normalization()(inputs)\n",
    "x = Dense(units=16, activation=\"relu\")(x)\n",
    "x = Dense(units=8, activation=\"relu\")(x)\n",
    "outputs = Dense(units=1, activation=\"sigmoid\")(x)\n",
    "cs1109_model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs1109_model.compile(optimizer=RMSprop(learning_rate=0.001), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs1109_model.fit(X_train_cs1109, y_train_cs1109, epochs=40, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = cs1109_model.evaluate(X_test_cs1109, y_test_cs1109)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiclass Classification on Irises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(4,))\n",
    "x = Normalization()(inputs)\n",
    "x = Dense(units=16, activation=\"relu\")(x)\n",
    "x = Dense(units=8, activation=\"relu\")(x)\n",
    "outputs = Dense(units=3, activation=\"softmax\")(x)\n",
    "iris_model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_model.compile(optimizer=RMSprop(learning_rate=0.001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_model.fit(X_train_iris, y_train_iris, epochs=40, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = iris_model.evaluate(X_test_iris, y_test_iris)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the loss function above: sparse_categorical_crossentropy for multiclass classification when the classes are integers, e.g. 0 = one kind of Iris, 1 = another kind, 2 = a third kind (which is what we have in the Iris dataset).\n",
    "\n",
    "Don't use categorical_cross_entropy. This is for when the *classes* have been one-hot encoded. This is not something we've been doing. Yes, we one-hot encoded the nominal-valued features - but not the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! We went straight from training (`fit`) to error estimation on the test set (`evaluate`). How do we do validation sets in Keras?\n",
    "\n",
    "The answer is that you can ask `fit` to split off some validation data. Let's illustrate on the housing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_history = housing_model.fit(X_train_housing, y_train_housing, validation_split=0.25, epochs=40, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in this example, it will train on 60% of the full dataset and, at the end of every epoch, it will test on 20%. (We heldout the remaining 20% as the test set.). \n",
    "\n",
    "So this uses holdout to get the validation set. There is no option for using k-fold cross-validation. You could write your own. But the assumption is that you are using the kind of large datasest where holdout is appropriate. (Of course, that isn't really true for the housing, CS1109 and Iris datasets.)\n",
    "\n",
    "`history` will be a dictionary that contains the loss at the end of every epoch and, in our case, the `mae` at the end of every epoch - or whatever `metrics` you asked for in `compile`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_keras_history(history, metric):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "    fig.tight_layout()\n",
    "    axes[0].plot(history.history[\"loss\"], label=\"train loss\")\n",
    "    axes[0].plot(history.history[\"val_loss\"], label=\"val loss\")\n",
    "    axes[0].set_title(\"Loss\")\n",
    "    axes[0].legend()\n",
    "    axes[1].plot(history.history[metric], label=\"train \" + metric)\n",
    "    axes[1].plot(history.history[\"val_\" + metric], label=\"val \" + metric)\n",
    "    axes[1].set_title(metric)\n",
    "    axes[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_keras_history(housing_history, \"mae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots help us detect problems.\n",
    "\n",
    "Question. Look at the left-hand plot. Why is this bad news?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once problems have been ironed out, you will be most interested in the final values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_err, val_err = housing_history.history[\"mae\"][-1], housing_history.history[\"val_mae\"][-1]\n",
    "train_err, val_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what about using the validation set to help us to choose the values of hyperparameters?\n",
    "\n",
    "A separate module (`keras_tuner`) makes this easier.\n",
    "\n",
    "Here's a simple example. First, we must build the model and compile it within a function - but we specify which parts are hyperparameters and which values we would like to try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_housing_model(hp):\n",
    "    inputs = Input(shape=(4,))\n",
    "    x = Normalization()(inputs)\n",
    "    x = Dense(units=16, activation=\"relu\")(x)\n",
    "    x = Dense(hp.Choice(\"units\", [2, 4, 8]), activation=\"relu\")(x)\n",
    "    outputs = Dense(units=1, activation=\"linear\")(x)\n",
    "    housing_model = Model(inputs, outputs)\n",
    "    housing_model.compile(optimizer=RMSprop(learning_rate=0.001), loss=\"mse\", metrics=[\"mae\"])\n",
    "    return housing_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = keras_tuner.GridSearch(\n",
    "    build_housing_model,\n",
    "    objective=\"val_mae\",\n",
    "    directory = base_dir,\n",
    "    project_name=\"tuner_state\",\n",
    "    overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(X_train_housing, y_train_housing, epochs=20, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.get_best_hyperparameters()[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_housing_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_housing_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = best_housing_model.fit(X_train_housing, y_train_housing, validation_split=0.25, epochs=40, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a second example in which there are more hyperparameters than in the previous example, and therefore we use a randomized seach instead of a grid-search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_housing_model(hp):\n",
    "    inputs = Input(shape=(4,))\n",
    "    x = Normalization()(inputs)\n",
    "    x = Dense(units=16, activation=\"relu\")(x)\n",
    "    hp_is_multilayered = hp.Boolean(\"is_multi_layered\")\n",
    "    if hp_is_multilayered:\n",
    "        x = Dense(hp.Choice(\"units\", [2, 4, 8]), activation=\"relu\")(x)\n",
    "    outputs = Dense(units=1, activation=\"linear\")(x)\n",
    "    housing_model = Model(inputs, outputs)\n",
    "    housing_model.compile(optimizer=hp.Choice(\"optimizer\", values =[\"sgd\", \"rmsprop\", \"adam\", \"nadam\"]), \n",
    "                          loss=\"mse\", metrics=[\"mae\"])\n",
    "    return housing_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = keras_tuner.RandomSearch(\n",
    "    build_housing_model,\n",
    "    objective=\"val_mae\",\n",
    "    max_trials=5,\n",
    "    directory = base_dir,\n",
    "    project_name=\"tuner_state\",\n",
    "    overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(X_train_housing, y_train_housing, epochs=20, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.get_best_hyperparameters()[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_housing_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_housing_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = best_housing_model.fit(X_train_housing, y_train_housing, validation_split=0.25, epochs=40, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification of Handwritten Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "MNIST is a classic dataset containing images of hand-written digits.\n",
    "\n",
    "Each example is a 28 pixel by 28 pixel grayscale image. The values are integers in [0, 255]. Each example is labelled to say what digit is contained in the image: 0-9. \n",
    "\n",
    "There are 70,000 images, so we can safely use holdout, and it is already partitioned: 60,000 training images; 10,000 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras has a utility function for downloading it into four Numpy arrays\n",
    "# To get this to work on macOS, you might need to run something like this in a terminal:\n",
    "# $ /Applications/Python\\ 3.12/Install\\ Certificates.command\n",
    "# You may need to rpelace 3.12 by whatever version of Python you are using, e.g. 3.13\n",
    "(X_train_mnist, y_train_mnist), (X_test_mnist, y_test_mnist) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mnist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mnist.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_train_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_mnist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_test_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 126 # Change this number to look at other images\n",
    "some_example = X_train_mnist[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the raw data for this image. Warning: large! (28 by 28)\n",
    "some_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Draw it\n",
    "some_example = some_example.reshape(28, 28)\n",
    "\n",
    "fig = plt.figure(figsize=(2,2))\n",
    "plt.imshow(some_example, cmap=plt.cm.binary, interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at its class\n",
    "y_train_mnist[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to reshape the data.\n",
    "\n",
    "Our training data is in a 3D array of shape (60000, 28, 28). We change it to a 2D array of shape (60000, 28 * 28). Similarly, the test data.\n",
    "\n",
    "This `flattens' the images. When working with images, it is often better not to do this. In a future lecture, we'll build neural networks that do not require us to flatten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mnist = X_train_mnist.reshape((60000, 28 * 28))\n",
    "\n",
    "X_test_mnist = X_test_mnist.reshape((10000, 28 * 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do a two-layer network. One hidden layer with 512 neurons, using the ReLU activation function. The output layer will have 10 neurons, one per class, and will use the softmax activation function.\n",
    "\n",
    "The features (pixel values) are all in the same range [0, 255], so we do not need to standardize using a Normalization layer. But it is a bad idea to feed into a neural network values that are much larger than the initial weights, so we will rescale to [0, 1] by dividing by 255. We can do this using a Rescaling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(28 * 28,))\n",
    "x = Rescaling(scale=1./255)(inputs)\n",
    "x = Dense(units=512, activation=\"relu\")(x)\n",
    "outputs = Dense(units=10, activation=\"softmax\")(x)\n",
    "mnist_model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model.compile(optimizer=RMSprop(learning_rate=0.0001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mnist_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you understand all the numbers in the table above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mnist_model.fit(X_train_mnist, y_train_mnist, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = mnist_model.evaluate(X_test_mnist, y_test_mnist)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having learned the weights, we should save the model (network, weights, training configuration, state of the optimizer) so that we don't have to learn them again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model.save(os.path.join(base_dir, \"models/my_model.keras\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reinstantiate a model, including compiling the model using the saved training configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = load_model(os.path.join(base_dir, \"models/my_model.keras\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
