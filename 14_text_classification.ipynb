{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec02c415-becb-4625-bd39-7eea8a052bb2",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac60507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795fefe8-5da7-46ef-9186-fee6b6a908d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d1a126-bede-420f-9c8b-64b65a18509b",
   "metadata": {},
   "source": [
    "## Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e169bf-366d-4c8f-abe4-bf87856f60f0",
   "metadata": {},
   "source": [
    "Researchers at Stanford University obtained 50,000 movie reviews from IMDB. They ensured an even number of positive and negative reviews. The positive reviews were ones where the author of the review had given the movie a rating of at least 7 out of 10. Negative reviews were ones where the author of the review had given the movie at most 4 out of 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc8e479-7ada-485f-923d-8f8f7a2ebc08",
   "metadata": {},
   "source": [
    "Question. Later in the Notebook, we will learn binary classifiers that achieve more than 80% accuracy on this dataset. Why does this figure tell us little about how our classifier would perform 'in the wild'? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2865eb1f-2ec1-4288-ab76-47afdd947438",
   "metadata": {},
   "source": [
    "We are using 25,000 of the reviews.\n",
    "\n",
    "They do not come as a nice CSV file. Each review is in a separate file. The labels come from the director structure: all the positive reviews are in a folder `pos`; all the negative ones are in a folder `neg`.\n",
    "\n",
    "scikit-learn has a function for reading in files from such a structure - `load_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95f3316-16af-4282-a5b1-10722f61eed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    base_dir = \"./drive/My Drive/Colab Notebooks/\" # You may need to change this, depending on where your notebooks are on Google Drive\n",
    "else:\n",
    "    base_dir = \".\"\n",
    "dataset_dir = os.path.join(base_dir, \"datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c69e3d-b81b-4f64-a4e3-8ae33582b60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = load_files(os.path.join(dataset_dir, \"reviews\"), encoding=\"utf-8\", random_state=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3d1821-bef7-4cea-bd16-25ce528a2d59",
   "metadata": {},
   "source": [
    "The result -which we have stored in `reviews`- is like a dictionary. `reviews.data` gives us a list of the reviews; `reviews.target` gives us a NumPy array of the class labels as integers; `reviews.target_names` maps the integers back to the class names. By default, the function shuffles the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecf76e4-0152-45e1-bf46-89b8be68c63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reviews.data), len(reviews.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522d10e6-b887-4781-8edc-6949f5b5dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.data[3] # Let's look at one example of a review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e90c03-9d7c-4509-985d-1b3ca71cde11",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31b35df-b70b-492e-9af9-32633c7d6732",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.target_names # So 0 is neg and 1 is pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2faa10-d140-4b79-909e-3374451763da",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.target.sum() / len(reviews.target) # Confirms that half the dataset are positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f23c4db-34b5-42a4-b6fb-3068176da270",
   "metadata": {},
   "source": [
    "Note `load_files` asks you for the `encoding` of the file. If you don't give one, it reads the file in as bytes instead of Unicode - and then a lot of other things won't work. For modern files, `encoding=\"utf-8\"` will probably work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f70b5c-e5b2-4654-8290-bfa0f1d1c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(reviews.data).duplicated().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b514167b-a6d8-44b0-9d62-726ef0296f8a",
   "metadata": {},
   "source": [
    "Question. There are 96 reviews that duplicate other reviews - too few to bother about. Really, I should delete them. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c3738-24c9-4b2a-80df-044da08d0ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reviews.data, reviews.target, test_size=0.2, stratify=reviews.target, random_state=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c857aff6-bf28-4604-8938-9f338eb53801",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42df76d-8005-4f93-8d45-441c9321e03e",
   "metadata": {},
   "source": [
    "The review we displayed earlier contained URLs and HTML tags. We don't want these to be tokens. Before we train any models, let's look at the kind of tokens that we'll be getting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe87ff9-3fb2-4f56-8ef6-1c9c8306dd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "vectorizer.fit(X_train)\n",
    "vectorizer.get_feature_names_out(), len(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb15bc87-47a9-428d-977a-fce6d8431b65",
   "metadata": {},
   "source": [
    "We can augment the sklearn preprocessor to strip away certain tokens, e.g. URLs, HTML and things starting with a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc436d23-f411-4067-b82c-aad1aed17e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class MovieReviewVectorizer(TfidfVectorizer):\n",
    "    \n",
    "    def build_preprocessor(self):\n",
    "        preprocess = super().build_preprocessor()\n",
    "        return lambda doc: (preprocess(self._strip_numerics(self._strip_urls(self._strip_html(doc)))))\n",
    "\n",
    "    def _strip_urls(self, s):\n",
    "        return re.sub(r\"http\\S+\", \"\", s) \n",
    "\n",
    "    def _strip_html(self, s):\n",
    "        return re.sub(r\"<.*>\", \"\", s)\n",
    "\n",
    "    def _strip_numerics(self, s):\n",
    "        return re.sub(r\"\\d\\S+\", \"\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee590df9-a841-4f4c-aa7e-e0683a7af198",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = MovieReviewVectorizer(stop_words=\"english\")\n",
    "vectorizer.fit(X_train)\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98e0bab-284c-4840-a160-588d1365de54",
   "metadata": {},
   "source": [
    "Rather than making our preprocessor better and better, we can just keep a subset of the tokens: the ones with highest term frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0ce1aa-9b99-4542-b708-0ac5661abc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = MovieReviewVectorizer(stop_words=\"english\", max_features=10000)\n",
    "vectorizer.fit(X_train)\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a531fff-b484-41f9-9a15-cf82ee96180d",
   "metadata": {},
   "source": [
    "In effect, this is a form of feature selection - using a filter method, where the scoring function that does the filtering is term frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131d2973-48d8-4d82-bdb2-0ef804950eaa",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad5df4-00d3-419d-a1d6-b43700db85da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = ShuffleSplit(n_splits=1, test_size=0.25, random_state=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342ee718-3b40-4f0f-977c-73377ba2b4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_fit(model, X_train, y_train, cv, metric):\n",
    "    scores = cross_validate(model, X_train, y_train, cv=cv, scoring=metric, return_train_score=True, n_jobs=-1)\n",
    "    return scores[\"train_score\"].mean(), scores[\"test_score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f1b208-db34-4560-9c81-76642ee8be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = Pipeline([\n",
    "    (\"vectorizer\", MovieReviewVectorizer(stop_words=\"english\", max_features=10000)),\n",
    "    (\"predictor\", LogisticRegression(penalty=None, random_state=rng))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8aa6cb-836d-46f6-8bbd-f16324a04675",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, val_acc = check_fit(model=logistic, \n",
    "            X_train=X_train, y_train=y_train, \n",
    "            cv=ss, metric=\"accuracy\")\n",
    "\n",
    "train_acc, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0685934-abe2-42a5-afbc-d6f5560eea3d",
   "metadata": {},
   "source": [
    "Question: Are we underfitting or overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dd030f-7461-4352-b189-9df3278d541d",
   "metadata": {},
   "source": [
    "I tried a few variants: retaining stop-words instead of discarding them, discarding a customized set of stop-words, count vectorization instead of TF-IDF, and so on. Their effect on validation accuracy was modest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27164bd7-4372-482b-994e-1775019415fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_svd = Pipeline([\n",
    "    (\"vectorizer\", MovieReviewVectorizer(stop_words=\"english\", max_features=10000)),\n",
    "    (\"svd\", TruncatedSVD(300)),\n",
    "    (\"predictor\", LogisticRegression(penalty=None, random_state=rng))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf9462-e526-4c49-9cf8-7e901bc68cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, val_acc = check_fit(model=logistic_svd, \n",
    "            X_train=X_train, y_train=y_train, \n",
    "            cv=ss, metric=\"accuracy\")\n",
    "\n",
    "train_acc, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadbbbc6-2f6a-4f5e-8ecc-eae62006aa24",
   "metadata": {},
   "source": [
    "Singular Value Decomposition (SVD) is what we use for this kind of dataset - in place of PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385c11c4-72d4-495b-9eb3-bcc2c98ac4f0",
   "metadata": {},
   "source": [
    "## n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2331b672-2365-4c2e-94e6-1d129c1b17e6",
   "metadata": {},
   "source": [
    "What kind of features would we get if we allowed bigrams? This time we won't discard stop-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22b0966-0d45-4a38-b0dc-532e9f430c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = MovieReviewVectorizer(ngram_range=(2,2), max_features=10000)\n",
    "vectorizer.fit(X_train)\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2894e3d8-a145-434a-bb7c-500bb1be591d",
   "metadata": {},
   "source": [
    "In the model, we'll allow both unigrams and bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835f0089-dca5-4234-a2ab-46d292d6486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_bigrams_svd = Pipeline([\n",
    "    (\"vectorizer\", MovieReviewVectorizer(ngram_range=(1,2), max_features=10000)),\n",
    "    (\"svd\", TruncatedSVD(300)),\n",
    "    (\"predictor\", LogisticRegression(penalty=None, random_state=rng))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc9c4c-911a-4aec-8c9d-1f7c72598f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, val_acc = check_fit(model=logistic_bigrams_svd, \n",
    "            X_train=X_train, y_train=y_train, \n",
    "            cv=ss, metric=\"accuracy\")\n",
    "\n",
    "train_acc, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39fe0d4-3b40-482a-98ba-d9dc87a4f2db",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64c2037-d5f1-4332-a446-ee27cbfd5237",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = Pipeline([\n",
    "    (\"vectorizer\", CountVectorizer(stop_words=\"english\", max_features=1000)),\n",
    "    (\"predictor\", MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ab26e8-f829-44c0-a4d9-7fe838d370b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, val_acc = check_fit(model=naive_bayes, \n",
    "            X_train=X_train, y_train=y_train, \n",
    "            cv=ss, metric=\"accuracy\")\n",
    "\n",
    "train_acc, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136930df-d9da-4f38-a3ef-e63a73cec63c",
   "metadata": {},
   "source": [
    "SVD does not make sense for Naive Bayes. But you'll see that I was able to filter more aggressivley (`max_features=1000`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bee075-e855-45bc-8d1e-1e82b71f3b29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
