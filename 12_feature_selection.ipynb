{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec02c415-becb-4625-bd39-7eea8a052bb2",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac60507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89b1baf-6b6f-4fac-8090-1e710581e042",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bccd14-0dd8-4412-bff7-816dcd81f27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(preprocessor, predictor, param_grid, cv, metric, X_train, y_train):\n",
    "    model = Pipeline([\n",
    "                (\"preprocessor\", preprocessor),\n",
    "                (\"predictor\", predictor)\n",
    "    ])\n",
    "\n",
    "    gs = GridSearchCV(model, param_grid, scoring=metric, cv=cv, n_jobs=-1)\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461fd6a9-fb88-4e1d-afed-6a26e6d01b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_fit(model, X_train, y_train, cv, metric):\n",
    "    scores = cross_validate(model, X_train, y_train, cv=cv, scoring=metric, return_train_score=True, n_jobs=-1)\n",
    "    return scores[\"train_score\"].mean(), scores[\"test_score\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7469813b-e0c5-45b1-8f34-c4b60d89fcbe",
   "metadata": {},
   "source": [
    "## Read in dataset, and split it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139d0d44-21c7-48c6-af92-748565c04458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    base_dir = \"./drive/My Drive/Colab Notebooks/\" # You may need to change this, depending on where your notebooks are on Google Drive\n",
    "else:\n",
    "    base_dir = \".\"\n",
    "dataset_dir = os.path.join(base_dir, \"datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19252902",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dataset_dir, \"glass.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808ef6ba-0f5c-49d2-8581-55b6b069fe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick look at what it contains\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b92fa44-b810-4d10-a548-2718e9e91073",
   "metadata": {},
   "source": [
    "Each row contains a unique id and then the refractive index and oxide content of a sheet of glass. We have a multiclass classification problem - to predict the Type. The Types have already been encoded as integers, so we do not need to do Label Encoding. They are as follows:\n",
    "- 1 float-processed glass for building windows\n",
    "- 2 non-float processed glass for building windows\n",
    "- 3 float-processed glass for vehicle windows\n",
    "- 4 non-float-processed glass for vehicle windows (none in this dataset)\n",
    "- 5 glass for containers\n",
    "- 6 glass for tableware\n",
    "- 7 glass for headlamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f7700e-8918-4219-912f-2c9f5cdaf9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"id\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20faaa8-e3e3-4c09-8954-4419caded3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c47d5d-7d28-43d7-b791-f6dab9d7c1ad",
   "metadata": {},
   "source": [
    "Since there are 10 features but just 214 rows, there is a likelihood of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6a0d58-d0f8-4756-80c3-963333277196",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, stratify=df[\"Type\"], random_state=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b6d86c-966b-449b-829e-7108de042b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array([\"RI\", \"Na\", \"Mg\", \"Al\", \"Si\", \"K\", \"Ca\", \"Ba\", \"Fe\"])\n",
    "\n",
    "X_train = train[features]\n",
    "y_train = train[\"Type\"]\n",
    "X_test = test[features]\n",
    "y_test = test[\"Type\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e20f11d-0d9d-4464-9304-a3608c492c69",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c7282c-51e4-4056-95f5-ad2294b4f531",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = Pipeline([\n",
    "        (\"preprocessor\", StandardScaler()),\n",
    "        (\"predictor\", LogisticRegression(penalty=None, max_iter=800, random_state=rng))\n",
    "    ])\n",
    "\n",
    "logistic_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe19348-9cfb-46ea-91c7-5607a65f3b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_fit(logistic_model, X_train, y_train, cv=5, metric=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bee92a-e55c-4f88-9dda-655ff4b1ee95",
   "metadata": {},
   "source": [
    "Overfitting! (Bear in mind this is accuracy, not error, so you have to turn your reasoning upside down.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab153a59-5769-4b2c-a411-9c92e52b7a9d",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5370eb93-cedf-47fc-a69b-1829b37bf56d",
   "metadata": {},
   "source": [
    "We discussed regularization of Linear Regression: Lasso Regression uses the $l_1$-norm; Ridge regression uses the $l_2$-norm. There is a hyperparameter $\\lambda$ to control the amount of regularization. scikit-learn has classes called `Lasso` and `Ridge` and it refers to $\\lambda$ as `alpha`! Increasing the value of `alpha` increases the amount of regularization.\n",
    "\n",
    "However, we are treating the prediction of wine quality as a multi-class classification problem, for which we would use Logistic Regression. But we can still regularize in a similar way. In scikit-learn, instead of having separate classes, the `LogisticRegression` class has an argument called `penalty`, whose values can be `None`, `l1`, `l2` (default) or `elasticnet` (which combines $l_1$ and $l_2$ regularization). One nuance is that, instead of `alpha`, we have `C`! Another nuance is that  `C` is the inverse of `alpha`: decreasing `C` increases the amount of regularization. A third nuance is that you must specify a solver and the default (`lbfgs`) only works for $l_2$ regularization. A solver that works for both is `saga`. Finally, it may not converge unless you specify a lot of iterations (`max_iter`, default 100) and relax the tolerance that determnes when it should stop (`tol`, default 0.0001). You may even need to switch to using `SGDClassifier` so that you can play with even more arguments such as the initial learning rate (`eta0`). Geez!\n",
    "\n",
    "Let's try it. Remember, we need to scale the data because Logistic Regression uses Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1096c7f0-7629-4cce-acbc-b26131056849",
   "metadata": {},
   "outputs": [],
   "source": [
    "regularized_logistic_gs = grid_search(\n",
    "    preprocessor = StandardScaler(), \n",
    "    predictor = LogisticRegression(solver=\"saga\", max_iter=800, random_state=rng),\n",
    "    param_grid = {\n",
    "        \"predictor__penalty\": [\"l1\", \"l2\"],\n",
    "        \"predictor__C\": np.linspace(0.1, 0.2, num=10)\n",
    "    },\n",
    "    cv = 5,\n",
    "    metric = \"accuracy\",\n",
    "    X_train = X_train,\n",
    "    y_train = y_train\n",
    ")\n",
    "\n",
    "regularized_logistic_gs.best_params_, regularized_logistic_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a678e31-f98c-482a-a71c-1afce824d75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_fit(regularized_logistic_gs.best_estimator_, X_train, y_train, cv=5, metric=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587e9f32-7bff-46c3-bdbf-388377d9e190",
   "metadata": {},
   "source": [
    "We've closed the gap!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae0368f-7d0e-47bf-bd53-6f4431e3b750",
   "metadata": {},
   "source": [
    "## Sequential Feature Selection Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfe6aa0-1e44-40fb-bcfe-e89396e7b716",
   "metadata": {},
   "source": [
    "This greedily inserts or discards a feature at a time - the one that does most or least to improve validation accuracy. In this case, it does this until it has only three features. On larger datasets, it will be slow because it must get the validation accuracy of many models to make its numerous decisions.\n",
    "\n",
    "First let's show what it does on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fff54a-d326-47cf-9659-d4b5e35a97f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs_preprocessor = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"f_selector\", SequentialFeatureSelector(estimator = LogisticRegression(penalty=None, max_iter=800, random_state=rng),\n",
    "                                             direction = \"forward\",\n",
    "                                             n_features_to_select = 4,\n",
    "                                             cv = 5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9e5a06-d310-423f-b7da-6fed43590363",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs_preprocessor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb12c63f-8f9e-41b2-aa55-4ee9a3c0cc99",
   "metadata": {},
   "source": [
    "Which four features did it retain? \n",
    "\n",
    "The `get_support` method of the `SequentialFeatureSelector` gives us a Boolean array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00265f0-39d0-4d67-b5fb-1aa03edd496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_selector = sfs_preprocessor.named_steps[\"f_selector\"]\n",
    "support = f_selector.get_support()\n",
    "features[support]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f508d77-bc02-42da-9208-d223a8581e79",
   "metadata": {},
   "source": [
    "So now let's make it the preprocessor in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261b5857-b30c-4c0d-ab82-5fe902ed23d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs_logistic_gs = grid_search(\n",
    "    preprocessor = sfs_preprocessor, \n",
    "    predictor = LogisticRegression(penalty=None, max_iter=800, random_state=rng),\n",
    "    param_grid = {},\n",
    "    cv = 5,\n",
    "    metric = \"accuracy\",\n",
    "    X_train = X_train,\n",
    "    y_train = y_train\n",
    ")\n",
    "\n",
    "sfs_logistic_gs.best_params_, sfs_logistic_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4440bfa-d8b3-4b59-b9c1-217df9dc8bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_fit(sfs_logistic_gs.best_estimator_, X_train, y_train, cv=5, metric=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ad86fd-216f-4479-8e92-0201f7e5d7cc",
   "metadata": {},
   "source": [
    "That's a substantial reduction in the size of the gap.\n",
    "\n",
    "Of course, the problem is: we don't know how many features to retain. Above, I arbitrarily chose 3.\n",
    "\n",
    "We can decide with a grid search - but it will build an awful lot of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa40159-ad7a-46f1-8969-f3b41d4f63f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs_logistic_gs = grid_search(\n",
    "    preprocessor = sfs_preprocessor, \n",
    "    predictor = LogisticRegression(penalty=None, max_iter=800, random_state=rng),\n",
    "    param_grid = {\n",
    "        \"preprocessor__f_selector__n_features_to_select\": range(1, 9)\n",
    "    },\n",
    "    cv = 5,\n",
    "    metric = \"accuracy\",\n",
    "    X_train = X_train,\n",
    "    y_train = y_train\n",
    ")\n",
    "\n",
    "sfs_logistic_gs.best_params_, sfs_logistic_gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118104d0-7a19-4bb7-9c93-0f023058e8f2",
   "metadata": {},
   "source": [
    "It finds that 3 features gives better validation accuracy than 4. \n",
    "\n",
    "Which features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1671e5f6-ae2f-42c6-b3b0-f1fa2eb57920",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_selector = sfs_logistic_gs.best_estimator_.named_steps[\"preprocessor\"].named_steps[\"f_selector\"]\n",
    "support = f_selector.get_support()\n",
    "features[support]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d377eced-7c86-4fa9-9ede-ebe9c8d9cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_fit(sfs_logistic_gs.best_estimator_, X_train, y_train, cv=5, metric=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c60e8cf-bcc9-41e8-b702-040ae6805f46",
   "metadata": {},
   "source": [
    "The gap with 4 features is about the same it was with 3.\n",
    "\n",
    "So now you could delete all the other features and retrain your model on just these four. Training and inference might now use less memory and take less time and you would be overfitting less than you were.\n",
    "\n",
    "Of course, the model still overfits. The obvious solution is: more training examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d736e0f2-6794-4345-9b1a-8a90a6f37442",
   "metadata": {},
   "source": [
    "## Filter Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ddbdd5-1ce9-440c-8e02-27d462f5ef42",
   "metadata": {},
   "source": [
    "We use scikit-learn's `SelectKBest` class. We must supply the function that measures feature importance. Lots of functions are already available. The two mentioned on the slides are F-value for regression (called `f_regression` in scikit-learn) and the ANOVA F-value for classification (called `f_classif`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6007dbc-c692-4c85-b774-29d419d6a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "skb_preprocessor = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"f_selector\", SelectKBest(score_func=f_classif, k=None))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fe9c16-72de-4d9a-94e2-78815c3f0427",
   "metadata": {},
   "outputs": [],
   "source": [
    "skb_logistic_gs = grid_search(\n",
    "    preprocessor = skb_preprocessor, \n",
    "    predictor = LogisticRegression(penalty=None, max_iter=800, random_state=rng),\n",
    "    param_grid = {\n",
    "        \"preprocessor__f_selector__k\": range(1, 9)\n",
    "    },\n",
    "    cv = 5,\n",
    "    metric = \"accuracy\",\n",
    "    X_train = X_train,\n",
    "    y_train = y_train\n",
    ")\n",
    "\n",
    "skb_logistic_gs.best_params_, skb_logistic_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703823e2-2cad-4c8e-8a6b-f94939785d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_selector = skb_logistic_gs.best_estimator_.named_steps[\"preprocessor\"].named_steps[\"f_selector\"]\n",
    "support = f_selector.get_support()\n",
    "features[support]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece55ecb-d3b6-4e1e-879a-d924b8e21bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_fit(skb_logistic_gs.best_estimator_, X_train, y_train, cv=5, metric=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d7d02e-7ddd-47e0-917c-fb05c1e5d585",
   "metadata": {},
   "source": [
    "It leaves us with a big gap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7c9d2c-084a-4a22-80e9-4681021f6a79",
   "metadata": {},
   "source": [
    "Instead of using a function to calculate feature importances, we can train a model that can output feature importances. We'll do it with a Random Forest.\n",
    "\n",
    "We won't use grid search to choose the number of Decision Trees inside the forest nor the maximum depth of those trees. Instead, we'll just use some small numbers: 100 and 2, respectively. This is for speed, but also because we aren't interested in finding a great Random Forest - just a decent estimate of feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de7259-0390-444f-864d-3eede16109bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfm_preprocessor = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"f_selector\", SelectFromModel(estimator=RandomForestClassifier(n_estimators=100, max_depth=2, random_state=rng), max_features=None))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a64a34d-8b32-4e7b-a4de-384f08d6c6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfm_logistic_gs = grid_search(\n",
    "    preprocessor = sfm_preprocessor, \n",
    "    predictor = LogisticRegression(penalty=None, max_iter=800, random_state=rng),\n",
    "    param_grid = {\n",
    "        \"preprocessor__f_selector__max_features\": range(1, 9)\n",
    "    },\n",
    "    cv = 5,\n",
    "    metric = \"accuracy\",\n",
    "    X_train = X_train,\n",
    "    y_train = y_train\n",
    ")\n",
    "\n",
    "sfm_logistic_gs.best_params_, sfm_logistic_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e46974f-1250-4640-926e-c80bfc4ff607",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_selector = sfm_logistic_gs.best_estimator_.named_steps[\"preprocessor\"].named_steps[\"f_selector\"]\n",
    "support = f_selector.get_support()\n",
    "features[support]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d487cac6-29e9-4630-9a1c-1aa20bc58673",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_fit(sfm_logistic_gs.best_estimator_, X_train, y_train, cv=5, metric=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bbca92-b055-49a0-bed5-97391e3d48a0",
   "metadata": {},
   "source": [
    "Not bad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378b9168-43ad-41af-8923-1679daac1e02",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a81548-a230-46b6-a979-13b6b13cff7b",
   "metadata": {},
   "source": [
    "Let's use PCA first and explain it afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f40d7b1-2013-4e64-9661-876f8ddcb0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_preprocessor = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"f_selector\", PCA(n_components=None, random_state=rng))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b48403-bfa1-41ff-9a65-f19cf7433b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_logistic_gs = grid_search(\n",
    "    preprocessor = pca_preprocessor, \n",
    "    predictor = LogisticRegression(penalty=None, max_iter=800, random_state=rng),\n",
    "    param_grid = {\n",
    "        \"preprocessor__f_selector__n_components\": range(1, 10)\n",
    "    },\n",
    "    cv = 5,\n",
    "    metric = \"accuracy\",\n",
    "    X_train = X_train,\n",
    "    y_train = y_train\n",
    ")\n",
    "\n",
    "pca_logistic_gs.best_params_, pca_logistic_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6add6d-8780-498f-9974-429c24fc59da",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_fit(pca_logistic_gs.best_estimator_, X_train, y_train, cv=5, metric=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d70c01a-22dc-4f3a-8eac-3a195643da36",
   "metadata": {},
   "source": [
    "Pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc833ed-3c17-4248-b32c-6dd5b1a53b47",
   "metadata": {},
   "source": [
    "Our grid search retained five components: five of the new features that replace the original features.\n",
    "\n",
    "In the previous parts of this Notebook, we were able to see which features were retained. But, with PCA, the components are not the same as the original features. They are linear combinations of the original features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b96ad-5f94-40b1-a376-043062c9c6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_selector = pca_logistic_gs.best_estimator_.named_steps[\"preprocessor\"].named_steps[\"f_selector\"]\n",
    "f_selector.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee26347e-364d-4a98-bf85-2c500f93f7c1",
   "metadata": {},
   "source": [
    "So the first component (new feature) is $0.56 \\times \\mathit{RI} + -0.22 \\times \\mathit{Na} + 0.13 \\times \\mathit{Mg} + -0.43 \\times \\mathit{Al} + -0.23 \\times \\mathit{Si} + -0.27 \\times \\mathit{K} + 0.50 \\times \\mathit{Ca} + -0.19 \\times \\mathit{Ba} + 0.18 \\times \\mathit{Fe}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0309cb2-23c0-4940-b42f-9c5d95d1be51",
   "metadata": {},
   "source": [
    "Above, we used a grid search to find how many new features to retain (`n_components`). \n",
    "\n",
    "Here is an alternative. We train a model that retains all of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71919584-a9b9-493d-b9b0-48564c751f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_logistic = Pipeline([\n",
    "    (\"preprocessor\", pca_preprocessor),\n",
    "    (\"predictor\", LogisticRegression(penalty=None, max_iter=800, random_state=rng))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab92477a-445e-4b59-86cf-8c0f3676a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_logistic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d84079c-ef98-48e7-8784-5477204fb132",
   "metadata": {},
   "source": [
    "Then, we can see how much of the variance each new feature 'explains':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73becca6-d2f5-425e-9c8a-076501d1380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_selector = pca_logistic.named_steps[\"preprocessor\"].named_steps[\"f_selector\"]\n",
    "f_selector.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a05975-fdb3-4d54-9b9d-e8abe9fd2495",
   "metadata": {},
   "source": [
    "Or, better, the percentage of the variance each 'explains':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e3cc39-cb07-45d0-8b60-b947f3a34f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_selector.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a04ea0-46c3-4863-a358-2e06c792a0f4",
   "metadata": {},
   "source": [
    "And we plot the explained variance ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aa12d5-19f4-4ac5-9a28-b46c64dfe559",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure() \n",
    "sns.lineplot(f_selector.explained_variance_ratio_)\n",
    "plt.xlabel(\"components\")\n",
    "plt.ylabel(\"explained variance ratio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e7e0bd-319e-4cc4-8f77-9712910f7310",
   "metadata": {},
   "source": [
    "Sometimes (but note here!), the plot falls steeply, then there is an 'elbow', after which the plot is flatter. The elbow gives the number of new features to retain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80aabc8-379e-4214-ac9d-f414ef153c91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
